server.port=9000

#Email helper address
emailHelperServer=http://172.18.1.24:9090
approversEmail=swami.sukhada@ishafoundation.org,maa.dakshina@ishafoundation.org,swami.vidhyavasanta@ishafoundation.org,maa.jeevapushpa@ishafoundation.org,dong.truong@ishafoundation.org

# Space info
remoteSshUsername=dongtruong
remoteSshPrivateKey=/Users/administrator/.ssh/id_rsa
remoteSshKeyPassword=
catDvIP=172.18.1.223
confluenceIP=172.18.1.222
localMountedOn=/data
catDvMountedOn=/data
confluenceMountedOn=/data
localFolderList=/data/user/dong.truong /data/user/pgurumurthy
catDvFolderList=/home/dongtruong/dwara /home/dongtruong/test

# Configuration for keycloak
dwara.keycloak.enabled=false
keycloak.realm=archives
keycloak.auth-server-url=http://172.18.1.222:8020/auth
keycloak.ssl-required=external
keycloak.resource=dwara
keycloak.credentials.secret=763c8899-df7b-46e0-b128-58fd08b0be8e
keycloak.confidential-port=0  

logging.level.org.ishafoundation.dwaraapi=trace
# logging.level.org.ishafoundation.dwaraapi.scheduler.ScheduledStatusUpdater=off
# logging.level.org.ishafoundation.dwaraapi.job.JobManager=off
# logging.level.org.ishafoundation.dwaraapi.storage.storagetype.StoragetypeJobDelegator=off
# logging.level.org.ishafoundation.dwaraapi.process.thread.ProcessingJobProcessor=off
# logging.level.org.ishafoundation.dwaraapi.utils.VolumeUtil=off
# logging.level.org.ishafoundation.dwaraapi.storage.storagetype.tape.job.TapeJobManager=off
# logging.level.org.ishafoundation.dwaraapi.resource.AutoloaderController=off
# logging.level.org.ishafoundation.dwaraapi.storage.storagetype.tape.TapeDeviceUtil=off
# logging.level.org.ishafoundation.dwaraapi.scheduler.ScheduledTapeUnloader=off  

appMode=online

# wire on wire off section - features wrapped with these wowo flags for testing waters/experimental mode 
wowo.useNewJobManagementLogic=true

# In Milliseconds
scheduler.jobManager.fixedDelay=30000
scheduler.statusUpdater.fixedDelay=30000

# daily - 24 * 60 * 60 * 1000
# hourly - 60 * 60 * 1000
scheduler.ingestedArtifactAutoDeleter.fixedDelay=3600000

# runs every 0th mt of an hour...
scheduler.tapeUnloader.cronExpression=0 0 * ? * *
scheduler.blankTapeAutoInitializer.cronExpression=0 0 0 ? * *


scheduler.autoIngester.cronExpression=0 0 0 ? * *
# max artifact count allowed to be ingested per schedule - If there are more artifacts to be ingested than the configured numbers we skip them current schedule
scheduler.autoIngester.maxArtifactCountPerSchedule=25
# If No. of jobs already queued is greater than the configured then we dont ingest any artifact...  
# How do we arrive at this number...
# what if at the time of ingest its 1 less than the configured and we ingest 25 artifacts which will increase the job count exponentially
scheduler.autoIngester.noOfJobsQueuedThreshold=70


# In minutes
scheduler.tapeUnloader.allowedTapeIdleSittingDuration=60

scheduler.statusUpdater.enabled=true
scheduler.jobManager.enabled=true
dwara.database.name=cp

server.error.whitelabel.enabled=false

# DB stuff
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.datasource.url=jdbc:mysql://localhost:3306/${dwara.database.name}?useSSL=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Kolkata
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.hikari.maximumPoolSize=20
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL57Dialect
# disabling Hibernate's auto schema generation
spring.jpa.hibernate.ddl-auto=update
# TODO This is an anti pattern according to https://vladmihalcea.com/the-hibernate-enable_lazy_load_no_trans-anti-pattern/
# While we are not bothered about performance we need to see if we can improve this like mentioned in
# https://vladmihalcea.com/the-best-way-to-map-a-projection-query-to-a-dto-with-jpa-and-hibernate/
# Dont understand whats recommended and so skipping the improved version - for now...
spring.jpa.properties.hibernate.enable_lazy_load_no_trans=true

# To warn when foldernames has special characters in it. Anything other than the Specified characters here shouldnt be there in the folder name
#\\w = Any word character, short for [a-zA-Z_0-9]
regexAllowedChrsInFileName=[\\w-.]*

stagingOpsScriptPath=/opt/dwara/bin/dwara-operations

setArtifactFileSystemPermissions=false
filesystem-permissions.owner=administrator
filesystem-permissions.group=wheel
filesystem-permissions.directoryMode=0775
filesystem-permissions.fileMode=0664
filesystem-permissions.recursive=true

junkFilesFinderRegexPatternList=\\._[^\\/]*$,\\.DS_Store$,\\.fseventsd$,\\..Spotlight-V100$,\\.TemporaryItems$,\\.Trashes$,\\.VolumeIcon.icns$,\\.fcpcache$,\\.AppleDouble$

junkFilesStagedDirName=.dwara-ignored

filesystem.rootlocation=C:\\data\\dwara
filesystem.temporarylocation=C:\\data\\tmp


readyToIngestSrcDirRoot=${filesystem.rootlocation}\\user
ingestCompleteDirRoot=${filesystem.rootlocation}\\ingested
# default retention period of ingest completed artifacts - in days
retentionPeriod=7
# artifactclass-specific override - 0 will delete immediately (or the next time the auto-delete process runs)
retentionPeriod.dvcapture-2020=0

commandlineExecutor.errorResponseTemporaryLocation=${filesystem.temporarylocation}

checksumType=sha256
checksumTypeSupportsStreamingVerification=true
encryptionAlgorithm=

# 1 KiB = 1024 bytes // 4 digits
# 1 MiB = 1048576 bytes // 7 digits // 1024 Ki
# 1 GiB = 1073741824 bytes // 10 digits // 1024 Mi
# 1 TiB = 1099511627776 bytes // 13 digits // 1024 Gi
# 6 TiB = 6597069766656 bytes

# LTO Tapes are in TB and not TiB
# 6 TB = 6000000000000 bytes (5.45 TiB)
# 5.7 TB to 5.88 TB
#volumeCapacity.watermarkLow=0.95
#volumeCapacity.watermarkHigh=0.98

# Test configuration for LTO 7 - 6 TB tapes
# 0.003333 - 20 GB - 20000000000
# 0.005 - 30 GB - 30000000000
# 0.01 - 60 GB - 60000000000
# 30 GB to 60 GB
# volumeCapacity.watermarkLow=0.005
# volumeCapacity.watermarkHigh=0.01

# Test configuration for LTO 6 - 2.5 TB tapes

# Test configuration for LTO 5 - 1.5 TB tapes
# 15 GB - 30 GB
volumeCapacity.watermarkLow=0.01
volumeCapacity.watermarkHigh=0.02


ffmpeg.video-proxy-low-gen.threads=2
ffmpeg.video-digi-2020-preservation-gen.threads=2
ffmpeg.video-digi-2020-qc-gen.threads=2

threadpoolexecutor.processingtask.corePoolSize=5
threadpoolexecutor.processingtask.maxPoolSize=5
threadpoolexecutor.processingtask.priority=5

threadpoolexecutor.video-mam-update.corePoolSize=1
threadpoolexecutor.video-mam-update.maxPoolSize=1
# threadpoolexecutor.video-mam-update.priority=5

sshPrvKeyFileLocation=C:\\Users\\prakash\\.ssh\\id_rsa

allowedAutoRequeueAttemptsOnFailedStorageJobs=2

# CatDV block
# For Constructing Server's URL
catdv.isSecured=false
catdv.host=localhost
catdv.port=8080

catdv.proxiesRootLocationSoftLinkName=mam

# SSH credentials
catdv.sshSystemUser=pgurumurthy
# the proxies location
catdv.sshProxiesRootLocation=C:\\data\\proxies

# Web UI Credentials
catdv.webUserID=Administrator
catdv.webUserPwd=

# These are the catalog Ids where the clip need to be inserted - To be revisited...
catdv.groupId.video-pub=2
catdv.groupId.video-priv1=358

non-dwara-audio-proxy.check=true
non-dwara-audio-proxy.remote=true
non-dwara-audio-proxy.localRootLocation=/data/non-dwara/AudioProxy
non-dwara-audio-proxy.host=172.18.1.24
non-dwara-audio-proxy.sshSystemUser=dwara
non-dwara-audio-proxy.sshRootLocation=/data/non-dwara/AudioProxy


# used when there are drives busy preventing mapping drives. The mapper waits for the below millis and then checks again if all the drives are available or not...
tapedrivemapping.pollinterval=60000

# the blocksize we will be using to write/read
tape.blocksize=256000

volume.label.ownerId=ISHAFOUNDATION

restoreTmpLocationForVerification=${filesystem.temporarylocation}/restoredforverification
restoreInProgressFileIdentifier=.restoring

allowForceOptionForTesting=true
checksumRsync=false