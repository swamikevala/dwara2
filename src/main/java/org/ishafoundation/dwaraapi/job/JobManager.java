package org.ishafoundation.dwaraapi.job;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

import org.ishafoundation.dwaraapi.db.attributeconverter.StoragetaskAttributeConverter;
import org.ishafoundation.dwaraapi.db.dao.master.TapeDao;
import org.ishafoundation.dwaraapi.db.dao.master.jointables.IngestconfigDao;
import org.ishafoundation.dwaraapi.db.dao.transactional.JobDao;
import org.ishafoundation.dwaraapi.db.dao.transactional.SubrequestDao;
import org.ishafoundation.dwaraapi.db.model.master.Tape;
import org.ishafoundation.dwaraapi.db.model.master.jointables.Ingestconfig;
import org.ishafoundation.dwaraapi.db.model.transactional.Job;
import org.ishafoundation.dwaraapi.db.model.transactional.Library;
import org.ishafoundation.dwaraapi.db.model.transactional.Request;
import org.ishafoundation.dwaraapi.db.model.transactional.Subrequest;
import org.ishafoundation.dwaraapi.enumreferences.Action;
import org.ishafoundation.dwaraapi.enumreferences.Status;
import org.ishafoundation.dwaraapi.enumreferences.Storagetask;
import org.ishafoundation.dwaraapi.enumreferences.Tasktype;
import org.ishafoundation.dwaraapi.process.thread.executor.StorageSingleThreadExecutor;
import org.ishafoundation.dwaraapi.process.thread.executor.TaskSingleThreadExecutor;
import org.ishafoundation.dwaraapi.process.thread.task.ProcessingtaskJobManager_ThreadTask;
import org.ishafoundation.dwaraapi.storage.thread.task.StorageJobsManager_ThreadTask;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.ApplicationContext;
import org.springframework.stereotype.Component;

@Component
public class JobManager {
	
	private static final Logger logger = LoggerFactory.getLogger(JobManager.class);

	@Autowired
	private IngestconfigDao ingestconfigDao;
	
	@Autowired
	private SubrequestDao subrequestDao;
	
	@Autowired
	private JobDao jobDao;	
		
	@Autowired
	private TapeDao tapeDao;
	
	@Autowired
	private JobUtils jobUtils;	
	
	@Autowired
	private TaskSingleThreadExecutor taskSingleThreadExecutor;
	
	@Autowired
	private StorageSingleThreadExecutor storageSingleThreadExecutor;

	@Autowired
	private ApplicationContext applicationContext;
	
	@Autowired
	private StoragetaskAttributeConverter storagetaskAttributeConverter;
	
	public List<Job> createJobsForIngest(Request request, Subrequest subrequest, Library library){
		List<Job> jobList = new ArrayList<Job>();

		//int tasktypeId = tasktypeDao.findByName(Tasktype.storage.name()).getId(); // TODO cache this...
		
		List<Ingestconfig> ingestconfigList = ingestconfigDao.findAllByInputLibraryclassIdOrderByDisplayOrderAsc(library.getLibraryclassId());
		for (Iterator<Ingestconfig> iterator = ingestconfigList.iterator(); iterator.hasNext();) {
			Ingestconfig ingestconfig = (Ingestconfig) iterator.next();
			
			Tasktype tasktype = ingestconfig.getTasktype();
			int taskId = ingestconfig.getTaskId();
			Job job = new Job();
			job.setTaskId(taskId);
			job.setTasktype(tasktype);
			// If a task contains prerequisite task that means its a derived one, for which the input library id needs to set by the prerequisite/parent task's job at the time of its processing and not at the time of job creation...
			// for eg., Mezz copy wont have a input library id upfront, which will be generated by its prerequisite/parent Mezz Transcoding job...
			if(ingestconfig.getPreProcessingTaskId() == null) {
				job.setInputLibrary(library);
			}
			
			if(tasktype == Tasktype.storage) {
				Tape tape = tapeDao.findTopByTapesetIdAndFinalizedIsFalseOrderByIdAsc(ingestconfig.getTapeset().getId());
				job.setTape(tape); // temporarily adding it. Even if the tape set here becomes full then, storagejobbuilder will size up and fill the right tape details...
			}
			job.setSubrequest(subrequest);				
			job.setCreatedAt(LocalDateTime.now());
			job.setStatus(Status.queued);
			job = saveJob(job);
			jobList.add(job);
			
			
			if(tasktype == Tasktype.storage) {
				if(taskId == 5) { // if storage task is archive... // TODO get the value from table or have it cached
					List<Job> archiveWrappedJobsList = createArchiveWrappedJobs(job);
					jobList.addAll(archiveWrappedJobsList);
				}
			}
		}
		
		return jobList;
	}
	
	private Job saveJob(Job job) {
		logger.debug("DB Job row Creation");   
		job = jobDao.save(job);
		logger.debug("DB Job row Creation - Success");
		return job;
	}
	
	private List<Job> createArchiveWrappedJobs(Job archiveJob){
		List<Job> jobList = new ArrayList<Job>();
		
		Job writeJob = createWriteJob(archiveJob);
		jobList.add(writeJob);
		
		List<Job> verifyAndItsWrappedJobsList = createVerifyAndItsWrappedJobs(archiveJob);
		jobList.addAll(verifyAndItsWrappedJobsList);
		
		return jobList;
	}
	
	private Job createWriteJob(Job archiveJob) {
		return createWrappedJob(archiveJob, 1);
	}
	
	private List<Job> createVerifyAndItsWrappedJobs(Job archiveJob) {
		List<Job> jobList = new ArrayList<Job>();
		Job verifyJob = createWrappedJob(archiveJob, 4);
		jobList.add(verifyJob);
		Job restoreJob = createWrappedJob(verifyJob, 2);
		jobList.add(restoreJob);
		Job verifyCrcJob = createWrappedJob(verifyJob, 3);
		jobList.add(verifyCrcJob);
		return jobList;
	}
	
	private Job createWrappedJob(Job wrapperJob, int taskId) {
		Job job = new Job();
		job.setJobRef(wrapperJob);
		job.setTaskId(taskId);
		job.setTasktype(wrapperJob.getTasktype());
		job.setInputLibrary(wrapperJob.getInputLibrary());
		job.setSubrequest(wrapperJob.getSubrequest());				
		job.setCreatedAt(LocalDateTime.now());
		job.setStatus(Status.queued);
		job = saveJob(job);
		return job;
	}
	
	public Job createJobForRestore(Request request, Subrequest subrequest){
		return createJobForNonIngestStorageTasks(request, subrequest);
	}	

	public Job createJobForLabeling(Request request, Subrequest subrequest){
		return createJobForNonIngestStorageTasks(request, subrequest);
	}
	
	private Job createJobForNonIngestStorageTasks(Request request, Subrequest subrequest){
		return createJobForNonIngestStorageTasks(request, subrequest, null);
	}
	
	private Job createJobForNonIngestStorageTasks(Request request, Subrequest subrequest, String storagetaskName){
		Job job = new Job();

		job.setTasktype(org.ishafoundation.dwaraapi.enumreferences.Tasktype.storage);
		if(storagetaskName == null)
			storagetaskName = request.getAction().name();
		Storagetask storagetask = org.ishafoundation.dwaraapi.enumreferences.Storagetask.valueOf(storagetaskName);
		int taskId = storagetaskAttributeConverter.convertToDatabaseColumn(storagetask);
		job.setTaskId(taskId);
		job.setSubrequest(subrequest);				
		job.setCreatedAt(LocalDateTime.now());
		job.setStatus(Status.queued);

		return job;
	}
	
	public void processJobs() {
		List<Job> storageJobList = new ArrayList<Job>();
		
		// Need to block all storage jobs when there is a queued/inprogress mapdrive request... 
		List<Status> statusList = new ArrayList<Status>();
		statusList.add(Status.queued);
		statusList.add(Status.in_progress);

		// If a subrequest action type is mapdrive and status is queued or inprogress skip storage jobs...
		long tapedrivemappingRequestInFlight = subrequestDao.countByRequestActionAndStatusIn(Action.map_tapedrives, statusList); 
		boolean isTapedrivemappingReqInFlight = false;
		if(tapedrivemappingRequestInFlight > 0)
			isTapedrivemappingReqInFlight = true;
		
		
		List<Job> jobList = jobDao.findAllByStatusAndJobRefIsNullOrderById(Status.queued); // Irrespective of the tapedrivemapping or format request non storage jobs can still be dequeued, hence we are querying it all... 
		List<Job> tapelabelingJobListQueued = jobDao.findAllBySubrequestRequestActionAndStatus(Action.format_tape, Status.queued);
		List<Job> tapelabelingJobListInProgress = jobDao.findAllBySubrequestRequestActionAndStatus(Action.format_tape, Status.in_progress);
		
		if(jobList.size() > 0) {
			for (Iterator<Job> iterator = jobList.iterator(); iterator.hasNext();) {
				Job job = (Job) iterator.next();
				logger.info("job - " + job.getId());
				int taskId = job.getTaskId();
				Tasktype tasktype = job.getTasktype();
				// check prerequisite job's completion status
				boolean isJobReadyToBeProcessed = false;
				
				//if(taskId == Storagetask.restore || taskId == Storagetask.format_tape) // TODO FIXME
				if(taskId == 2 || taskId == 7)
					isJobReadyToBeProcessed = true;
				else
					isJobReadyToBeProcessed = isJobReadyToBeProcessed(job);
				
				logger.info("isJobReadyToBeProcessed - " + isJobReadyToBeProcessed);
				if(isJobReadyToBeProcessed) {
					// TODO : we were doing this on tasktype, but now that there is no tasktype how to differentiate? Check with Swami
					if(tasktype == Tasktype.processing) { // a non-storage process job
						logger.trace("process job");
						ProcessingtaskJobManager_ThreadTask taskJobManager_ThreadTask = applicationContext.getBean(ProcessingtaskJobManager_ThreadTask.class);
						taskJobManager_ThreadTask.setJob(job);
						taskSingleThreadExecutor.getExecutor().execute(taskJobManager_ThreadTask);
					}else {
						if(isTapedrivemappingReqInFlight) { // there is a queued map drive request, so blocking all storage jobs until the mapdrive request is complete...
							logger.trace("Skipping adding to storagejob collection as Tapedrivemapping InFlight");
						}
						else if(tapelabelingJobListInProgress.size() > 0) { // if any tape labeling request already in flight
							logger.trace("Skipping adding to storagejob collection as Tapelabeling InFlight");
						}
						else if(tapelabelingJobListQueued.size() > 0) { // if any tape labeling request queued up
							// only adding the tape labeling job to the list
							if(job.getSubrequest().getRequest().getAction() == Action.format_tape) {
								if(storageJobList.size() == 0) { // add only one job at a time. If already added skip it
									storageJobList.add(job);
									logger.trace("Added the format job to storagejob collection");
								}
								else
									logger.trace("Already another format job added to storagejob collection. So skipping this");
							}
						}
						else { // only add when no tapedrivemapping or format activity
							// all storage jobs need to be grouped for some optimisation...
							storageJobList.add(job);
							logger.trace("Added to storagejob collection");
						}
					}
				}
			}
			
			if(storageJobList.size() > 0) {
				StorageJobsManager_ThreadTask storageThreadTask = applicationContext.getBean(StorageJobsManager_ThreadTask.class);
				storageThreadTask.setJobList(storageJobList);
				storageSingleThreadExecutor.getExecutor().execute(storageThreadTask);
			}else {
				logger.trace("No storage job to be processed");
			}
		}
		else {
			logger.trace("No jobs queued up");
		}
	}

	// If a job is a dependent job the parent job's status should be completed for it to be ready to be taken up for processing...
	private boolean isJobReadyToBeProcessed(Job job) {
		boolean isJobReadyToBeProcessed = true;
		
		Library inputLibrary = job.getInputLibrary();//The input library of a dependent job is set by the parent job after it completes the processing
		if(inputLibrary == null) { // means its a job dependent on its parent job(to set the library to be used), which is not completed.  
			isJobReadyToBeProcessed = false;
		}
		else {
			Job parentJob = jobUtils.getPrerequisiteJob(job); // Has the prerequisite job's status is complete?
			if(parentJob != null) { 
				// means a dependent job.
				Status parentJobStatus = parentJob.getStatus();
				if(parentJobStatus != Status.completed && parentJobStatus != Status.completed_failures)
					isJobReadyToBeProcessed = false;
			}
		}
		return isJobReadyToBeProcessed;
	}
	

}
	
